{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNn6t83svFFow3mfwNpVU59"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Base Model**"],"metadata":{"id":"qKbwxwrWIMYH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVdxhTxxIIut"},"outputs":[],"source":["!mkdir sample_gen\n","!mkdir model"]},{"cell_type":"markdown","source":["## **Import Library**"],"metadata":{"id":"Mw6KWJQ5ISIC"}},{"cell_type":"code","source":["import numpy as np\n","from numpy.random import default_rng\n","import pandas as pd\n","import h5py\n","from matplotlib import pyplot as plt\n","\n","import tensorflow as tf\n","from keras.initializers import RandomNormal\n","from keras import Input, activations\n","from keras.models import Model\n","from keras.layers import Dense\n","from keras.layers import Concatenate\n","from keras.layers import Conv2D, Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.layers import Dropout\n","from keras.layers import Activation\n","from keras.backend as kb\n","from keras.losses import mean_absolute_error\n","\n","# from keras.layers import Flatten\n","\n","from keras.optimizers import Adam\n","from keras.utils.vis_utils import plot_model\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n"],"metadata":{"id":"U25y0lcZIW9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Set main parameters**"],"metadata":{"id":"oNqDjL7jIZLt"}},{"cell_type":"code","source":["# for images\n","img_width = 256\n","img_height = 256\n","img_channels = 3\n","\n","img_shape = (img_width, img_height, img_channels)\n","\n","batches = 10"],"metadata":{"id":"Vq1zLcFuIeNs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Build Generator**"],"metadata":{"id":"R05yM7leIfWT"}},{"cell_type":"markdown","source":["### Encoder"],"metadata":{"id":"7Xv3UColIi_E"}},{"cell_type":"code","source":["def encoder(prev_layer, n_filters, n_kernels=4, n_strides=1, do_batchNorm=True):\n","  # weight initialization\n","  init = RandomNormal(stddev=0.02) use_bias=False\n","  \n","  use_bias = False if do_batchNorm == True else True\n","\n","  encoder = Conv2D(n_filters, (n_kernels,n_kernels), strides=(n_strides,n_strides), padding=\"same\", kernel_initializer=init, use_bias=use_bias)(prev_layer)\n","  if do_batchNorm == True:\n","    encoder = BatchNormalization()(encoder, training=True)\n","  encoder = LeakyReLU(alpha=0.2)(encoder)\n","\n","  return encoder"],"metadata":{"id":"34Z4wNQhIhrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decoder"],"metadata":{"id":"wGxPlEnPIox_"}},{"cell_type":"code","source":["def decoder(prev_layer, skip_layer, n_filters, n_kernels=4, n_strides=1, do_dropout=True):\n","  # weight initialization\n","  init = RandomNormal(stddev=0.02)\n","\n","  decoder = Conv2DTranspose(n_filters, (n_kernels,n_kernels), strides=(n_strides,n_strides), padding=\"same\", kernel_initializer=init, use_bias=False)(prev_layer)\n","  decoder = BatchNormalization()(decoder, training=True)\n","  if do_dropout == True:\n","    decoder = Dropout(0.5)(decoder, training=True)\n","  decoder = Concatenate()([decoder, skip_layer])\n","  decoder = Activation(activations.relu)(decoder)\n","\n","  return decoder"],"metadata":{"id":"uDfxUBcSIqB_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generator"],"metadata":{"id":"9Fk5cMsLIsCP"}},{"cell_type":"code","source":["def build_generator(img_shape=img_shape, n_kernels=4, n_strides=1):\n","  # weight initialization\n","  init = RandomNormal(stddev=0.02)\n","\n","  input_layer = Input(img_shape)\n","\n","  # encoders\n","  encoder_1 = encoder(input_layer, 64, n_kernels=n_kernels, n_strides=n_strides, do_batchNorm=False)\n","  encoder_2 = encoder(encoder_1, 128, n_kernels=n_kernels, n_strides=n_strides)\n","  encoder_3 = encoder(encoder_2, 256, n_kernels=n_kernels, n_strides=n_strides)\n","  encoder_4 = encoder(encoder_3, 512, n_kernels=n_kernels, n_strides=n_strides)\n","  encoder_5 = encoder(encoder_4, 512, n_kernels=n_kernels, n_strides=n_strides)\n","  encoder_6 = encoder(encoder_5, 512, n_kernels=n_kernels, n_strides=n_strides)\n","  encoder_7 = encoder(encoder_6, 512, n_kernels=n_kernels, n_strides=n_strides)\n","\n","  bottleneck = Conv2D(512, (n_kernels, n_kernels), strides=(n_strides, n_strides), padding=\"same\", kernel_initializer=init)(encoder_7)\n","  bottleneck = Activation(activations.relu)(bottleneck)\n","\n","  # decoders\n","  decoder_1 = decoder(bottleneck, encoder_7, 512, n_kernels=n_kernels, n_strides=n_strides)\n","  decoder_2 = decoder(decoder_1, encoder_6, 512, n_kernels=n_kernels, n_strides=n_strides)\n","  decoder_3 = decoder(decoder_2, encoder_5, 512, n_kernels=n_kernels, n_strides=n_strides)\n","  decoder_4 = decoder(decoder_3, encoder_4, 512, n_kernels=n_kernels, n_strides=n_strides, do_dropout=False)\n","  decoder_5 = decoder(decoder_4, encoder_3, 256, n_kernels=n_kernels, n_strides=n_strides, do_dropout=False)\n","  decoder_6 = decoder(decoder_5, encoder_2, 128, n_kernels=n_kernels, n_strides=n_strides, do_dropout=False)\n","  decoder_7 = decoder(decoder_6, encoder_1, 64, n_kernels=n_kernels, n_strides=n_strides, do_dropout=False)\n","\n","  output_layer = Conv2DTranspose(3, (n_kernels, n_kernels), strides=(n_strides,n_strides), padding='same', kernel_initializer=init)(decoder_7)\n","  output_layer = Activation(activations.tanh)(output_layer)\n","\n","  model = Model(inputs=input_layer, outputs=output_layer, 'generator')\n","  return model\n"],"metadata":{"id":"zXv7IG-dIrd4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Build Discriminator**"],"metadata":{"id":"QPf9moeEIy2C"}},{"cell_type":"code","source":["def build_discriminator(img_shape=img_shape, n_kernel=4, n_strides=1):\n","\n","  # weight initialization\n","  init = RandomNormal(stddev=0.02)\n","\n","  # src_input = Sequential()\n","  src_input = Input(shape=img_shape)\n","\n","  # target_input = Sequential()\n","  target_input = Input(shape=img_shape)\n","\n","  concat_input = Concatenate()([src_input, target_input])\n","\n","  layer = Conv2D(64, (n_kernel,n_kernel), strides=(n_strides,n_strides), padding=\"same\", kernel_initializer=init)(concat_input)\n","  layer = LeakyReLU(alpha=0.2)(layer)\n","\n","  layer = Conv2D(128, (n_kernel,n_kernel), strides=(n_strides,n_strides), padding=\"same\", kernel_initializer=init, use_bias=False)(layer)\n","  layer = BatchNormalization()(layer)\n","  layer = LeakyReLU(alpha=0.2)(layer)\n","\n","  layer = Conv2D(256, (n_kernel,n_kernel), strides=(n_strides,n_strides), padding=\"same\", kernel_initializer=init, use_bias=False)(layer)\n","  layer = BatchNormalization()(layer)\n","  layer = LeakyReLU(alpha=0.2)(layer)\n","\n","  layer = Conv2D(512, (n_kernel,n_kernel), strides=(n_strides,n_strides), padding=\"same\", kernel_initializer=init, use_bias=False)(layer)\n","  layer = BatchNormalization()(layer)\n","  layer = LeakyReLU(alpha=0.2)(layer)\n","  \n","  layer = Conv2D(512, (n_kernel,n_kernel), padding=\"same\", kernel_initializer=init, use_bias=False)(layer)\n","  layer = BatchNormalization()(layer)\n","  layer = LeakyReLU(alpha=0.2)(layer)\n","\n","  layer = Conv2D(1, (n_kernel,n_kernel), padding=\"same\", kernel_initializer=init)(layer)\n","\n","  ## Flatten\n","  # layer = Flatten()(layer)\n","  ## Dropout\n","  # layer = Dropout(0.5)(layer)\n","\n","  out_layer = Activation(activations.sigmoid)(layer)\n","\n","  model = Model(inputs=[src_input, target_input], outputs=out_layer, name='discriminator')\n","  \n","  return model\n"],"metadata":{"id":"MI7sGH8-I1Id"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Connecting generator and discriminator to build GAN**"],"metadata":{"id":"-w-iX7X9I3A8"}},{"cell_type":"code","source":["def build_gan(generator, discriminator, img_shape=img_shape):\n","  discriminator.trainable = False\n","\n","  input_layer = Input(shape=img_shape)\n","  \n","  generator_layer = generator(input_layer)\n","\n","  discriminator_layer = discriminator([input_layer, generator_layer])\n","\n","  model = Model(inputs=input_layer, outputs=[discriminator_layer, generator_layer], name='GAN')\n","  \n","  return model\n"],"metadata":{"id":"pfF1CszoI65X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Save model architecture images**"],"metadata":{"id":"7RWfEs-TJCkC"}},{"cell_type":"code","source":["def save_model_architecture_image(model, name):\n","  plot_model(model, to_file=f\"{name}_architecture.png\", show_shapes=True, show_layer_names=True)\n","\n","generator = build_generator(n_strides=2)\n","discriminator = build_discriminator(n_strides=2)\n","gan = build_gan(generator, discriminator)\n","\n","save_model_architecture_image(generator, \"generator\")\n","save_model_architecture_image(discriminator, \"discriminator\")\n","save_model_architecture_image(gan, \"gan\")"],"metadata":{"id":"4eZQ47fxJHyd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Loss Function**"],"metadata":{"id":"EDgdjXptNMXx"}},{"cell_type":"code","source":["def pixel_loss(y_true, y_pred):\n","  return kb.mean(kb.abs(y_true - y_pred))\n","\n","\n","def contextual_loss(y_true, y_pred):\n","  a = tf.image.rgb_to_grayscale(tf.slice(y_pred, [0, 0, 0, 0], [batches, 256, 256, 3]))\n","  b = tf.image.rgb_to_grayscale(tf.slice(y_true, [0, 0, 0, 0], [batches, 256, 256, 3]))\n","\n","  y_pred = tf.divide(tf.add(tf.reshape(a, [tf.shape(a)[0], -1]), 1), 2)\n","  y_true = tf.divide(tf.add(tf.reshape(b, [tf.shape(b)[0], -1]), 1), 2)\n","\n","  p_shape = tf.shape(y_true)\n","  q_shape = tf.shape(y_pred)\n","\n","  p_ = tf.divide(y_true, tf.tile(tf.expand_dims(tf.reduce_sum(y_true, axis=1), 1), [1,p_shape[1]]))\n","  q_ = tf.divide(y_pred, tf.tile(tf.expand_dims(tf.reduce_sum(y_pred, axis=1), 1), [1,p_shape[1]]))\n","    \n","  return tf.reduce_sum(tf.multiply(p_, tf.log(tf.divide(p_, q_))), axis=1)\n","\n","\n","def total_loss(y_true, y_pred):\n","  pix_loss = pixel_loss(y_true, y_pred):\n","  cont_loss = contextual_loss(y_true, y_pred)\n","  return (0.2 * pix_loss) + (0.8 * cont_loss)"],"metadata":{"id":"5qxMO80cNPiN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Prepare before training**\n","\n","label\n","\n","1: real\n","\n","0: fake"],"metadata":{"id":"W97rORe0JJu4"}},{"cell_type":"markdown","source":["## **Prepare Data**"],"metadata":{"id":"OBAeqKpKJLYE"}},{"cell_type":"code","source":["# get data from h5 file\n","def load_data(file_name):\n","  with h5py.File(file_name, \"r+\") as file:\n","    images = np.array(file['/images']).astype('uint8')\n","    sketches = np.array(file['/sketches']).astype('uint8')\n","\n","    # convert to 3 channels\n","    sketches = np.stack((sketches,)*3, axis=-1)\n","    \n","    # (0,255) -> (-1,1)\n","    images = (images / 127.5) - 1\n","    sketches = (sketches / 127.5) - 1\n","\n","  return images, sketches\n","\n","# random pairs of images for a batch\n","def get_real_sample_images_data(images, sketches, n_samples, n_patches=1, seed=None):\n","  # random instance\n","  rnd = default_rng(seed=seed)\n","  rand_i = rnd.choice(images.shape[0], n_samples, replace=False)\n","  X_images, X_sketches = images[rand_i], sketches[rand_i]\n","\n","  # add label 1\n","  y = np.ones((n_samples, n_patches, n_patches, 1))\n","\n","  return X_images, X_sketches, y\n","\n","def generate_sample_fake_data(generator, samples, n_patches=1):\n","  # generate fake images\n","  X = generator.predict(samples)\n","\n","  # add label 0\n","  y = np.zeros((len(X), n_patches, n_patches, 1))\n","\n","  return X, y"],"metadata":{"id":"ijS0XNKVJK4q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Summarize**"],"metadata":{"id":"H6V386jsJQ1B"}},{"cell_type":"code","source":["def rescale(images):\n","  # (-1,1) -> (0,1) for matplotlib\n","  return (images + 1) / 2\n","\n","def summarize(iteration, generator, images, sketches, n_samples):\n","  # real\n","  X_images, X_sketches, _ = get_real_sample_images_data(images, sketches, n_samples, seed=42)\n","\n","  # generate fake images\n","  X_fake_images, _ = generate_sample_fake_data(generator, X_sketches)\n","  \n","  plt.figure(figsize=(20,12))\n","\n","  X_sketches = rescale(X_sketches)\n","  X_images = rescale(X_images)\n","  X_fake_images = rescale(X_fake_images)\n"," \n","  # same sample images\n","  for i in range(n_samples):\n","    sketches_ax = plt.subplot2grid((3,n_samples), (0,i))\n","    real_ax = plt.subplot2grid((3,n_samples), (1,i))\n","    gen_ax = plt.subplot2grid((3,n_samples), (2,i))\n","\n","    sketches_ax.set_xticks([])\n","    sketches_ax.set_yticks([])\n","    real_ax.set_xticks([])\n","    real_ax.set_yticks([])\n","    gen_ax.set_xticks([])\n","    gen_ax.set_yticks([])\n","\n","    if i == 0:\n","      sketches_ax.set_ylabel(\"Sketches\", fontsize=20)\n","      real_ax.set_ylabel(\"Real images\", fontsize=20)\n","      gen_ax.set_ylabel(\"Generated images\", fontsize=20)\n","\n","    sketches_ax.imshow(X_sketches[i])\n","    real_ax.imshow(X_images[i][...,::-1])\n","    gen_ax.imshow(X_fake_images[i][...,::-1])\n","\n","  plt.savefig(f\"/content/sample_gen/sample_{str(iteration+1).rjust(7,'0')}.png\")\n","\n","  # generator.save(f\"/content/model/generator_{str(iteration+1).rjust(7,'0')}.h5\")\n"],"metadata":{"id":"c8mZ-KC4JUDM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_history(list_d_loss1, list_d_loss2, list_g_loss, list_d_acc1, list_d_acc2):\n","  plt.subplot(311)\n","  plt.plot(list_g_loss, label=\"g_loss\")\n","  plt.xlabel(\"iteration\")\n","  plt.ylabel(\"loss\")\n","  plt.legend()\n","\n","  plt.subplot(312)\n","  plt.plot(list_d_loss1, label=\"d_loss1\")\n","  plt.plot(list_d_loss2, label=\"d_loss2\")\n","  plt.xlabel(\"iteration\")\n","  plt.ylabel(\"loss\")\n","  plt.legend()\n","  \n","  plt.subplot(313)\n","  plt.plot(list_d_acc1, label=\"d_acc1\")\n","  plt.plot(list_d_acc2, label=\"d_acc2\")\n","  plt.xlabel(\"iteration\")\n","  plt.ylabel(\"accuracy\")\n","  plt.legend()"],"metadata":{"id":"ER4l_nB_JW4a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Train**\n","d_loss1: discriminator (real images)\n","\n","d_loss2: discriminator (generated images)\n","\n","g_loss: generator"],"metadata":{"id":"88I-Zfr9JX_9"}},{"cell_type":"code","source":["def training(generator, discriminator, gan, images, sketches, epochs=100, batches=1, data):\n","\n","  n_patches = discriminator.output_shape[1]\n","\n","  # number of batches per epoch\n","  batches_per_epoch = int(len(sketches) / batches) \n","  n_iterations = batches_per_epoch * epochs\n","\n","  list_d_loss1 = []\n","  list_d_loss2 = []\n","  list_g_loss = []\n","  list_d_acc1 = []\n","  list_d_acc2 = []\n","\n","  for i in range(n_iterations):\n","\n","    X_images, X_sketches, y_real = get_real_sample_images_data(images, sketches, batches, n_patches)\n","\n","    X_fake_images, y_fake = generate_sample_fake_data(generator, X_sketches, n_patches)\n","\n","    d_loss1, d_acc1 = discriminator.train_on_batch([X_sketches, X_images], y_real)\n","    d_loss2, d_acc2 = discriminator.train_on_batch([X_sketches, X_fake_images], y_fake)\n","    g_loss, _, _ = gan.train_on_batch(X_sketches, [y_real, X_images])\n","\n","    print(\">>> iteration %d, g_loss: %.3f d_loss1: %.3f d_loss2: %.3f d_acc1: %.3f d_acc2: %.3f\" % (i+1, g_loss, d_loss1, d_loss2, d_acc1, d_acc2))\n","\n","    list_d_loss1.append(d_loss1)\n","    list_d_loss2.append(d_loss2)\n","    list_g_loss.append(g_loss)\n","    list_d_acc1.append(d_acc1)\n","    list_d_acc2.append(d_acc2)\n","\n","\n","    if (i+1) % (batches_per_epoch * 10) == 0 or i in [0, 1]:\n","      summarize(i, generator, images, sketches, 5)\n","  \n","  plot_history(list_d_loss1, list_d_loss2, list_g_loss, list_d_acc1, list_d_acc2)\n","\n","  # save model\n","  # generator.save(f'/content/drive/MyDrive/trained_model/generator_{data}.h5')"],"metadata":{"id":"sNgjLtSdJZD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images, sketches = load_data(\"/content/drive/MyDrive/data/test0_images.h5\")\n","generator = build_generator(n_strides=2)\n","discriminator = build_discriminator(n_strides=2)\n","gan = build_gan(generator, discriminator)\n","\n","\n","adam = Adam(learning_rate=0.0002, beta_1=0.5)\n","\n","# set batch add \"Set main parameter\"\n","discriminator.compile(loss='binary_crossentropy', optimizer=adam, loss_weights=[0.5], metrics=['accuracy'])\n","gan.compile(loss=['binary_crossentropy', total_loss], optimizer=adam, loss_weights=[1,100])\n","\n","training(generator, discriminator, gan, images, sketches, batches=batches, data=0)"],"metadata":{"id":"sbXmSb4KJbrA"},"execution_count":null,"outputs":[]}]}